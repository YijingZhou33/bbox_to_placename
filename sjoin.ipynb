{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populating Spatial Coverage for DCAT Data Portals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction\n",
    "This Jupyter Notebook is intended to find the spatial coverage based on bounding box for DCAT Data Portals. It is a reverse version of **<a href='https://github.com/BTAA-Geospatial-Data-Project/geonames'>geonames</a>**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparation\n",
    "We will be using **Jupyter Notebook(anaconda 3)** to edit and run the script. Information on Anaconda installation can be found <a href='https://docs.anaconda.com/anaconda/install/'>here</a>. Please note that this script is running on Python 3.\n",
    "\n",
    "Before running the script, you may need to:\n",
    "### 1. Inspect the csv file\n",
    "Check if it includes all the required columns with proper name\n",
    "- required columns\n",
    "    1. Download\n",
    "    2. Slug\n",
    "    3. Publisher\n",
    "    4. Code\n",
    "    5. Bounding Box\n",
    "    \n",
    "### 2. Run other Jupyter Notebooks\n",
    "- If the target state(s) hasn't been converted into city or county bounding box file, you may need to \n",
    "    1. download county and city boundary file (GeoJSON or Shapefile) online\n",
    "    2. run `city_boundary.ipynb` or `county_boundary.ipynb` to create boundary GeoJSON files \n",
    "        - if there exists regional data portals, you may need to run `merge_geojson.ipynb` to merge them together\n",
    "    3. run `city_bbox.ipynb` or `county_bbox.ipynb` to create bounding box GeoJSON files\n",
    "\n",
    "### 3. Restructure Directories\n",
    "- `sjoin.ipynb`\n",
    "- **geojsonScripts** folder\n",
    "    - `city_boundary.ipynb`\n",
    "    - `county_boundary.ipynb`\n",
    "    - `city_bbox.ipynb`\n",
    "    - `county_bbox.ipynb`\n",
    "    - `merge_geojsons.ipynb`\n",
    "- **geojsons** folder\n",
    "    - **State** foloder\n",
    "        - *State_County_boundaries.json*\n",
    "        - *State_City_boundaries.json*\n",
    "        - *State_County_bbox.json*\n",
    "        - *State_City_bbox.json*\n",
    "    - **portalCode** foloder (Multiple states)\n",
    "        - *portalCode_County_boundaries.json*\n",
    "        - *portalCode_City_boundaries.json*\n",
    "        - *portalCode_County_bbox.json*\n",
    "        - *portalCode_City_bbox.json*\n",
    "    - ...\n",
    "- **reports** folder\n",
    "    - one csv file formatted in GBL Metadata Template\n",
    "        \n",
    "The final product would be one CSV file. \n",
    "\n",
    "> Original created on Feb 4 2021 <br>\n",
    "> @author: Yijing Zhou @YijingZhou33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from shapely.geometry import box\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from itertools import repeat\n",
    "from functools import reduce\n",
    "import time\n",
    "import urllib.request\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ActionDate = time.strftime('%Y%m%d')\n",
    "# newItemscsv = os.path.join('reports', f'allNewItems_{ActionDate}.csv')\n",
    "\n",
    "##### CSV file with metadata #####\n",
    "csvname = 'imagery'\n",
    "newItemscsv = os.path.join('reports', csvname + '.csv')\n",
    "df_csv = pd.read_csv(newItemscsv, encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Check if link is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--------------------- Check if link is valid: --------------------\\n\")\n",
    "\n",
    "def check_url(df, timeout):\n",
    "    totalcount = len(df.index)\n",
    "    countnotshp = countprivate = countok = count404 = count500 = countothercode = countconnection = counttimeout = 0\n",
    "    start_time = time.time()\n",
    "    filesize = download = imageserver = None\n",
    "    filesizelist, downloadlist, imageserverlist, oklist, checkagainlist = ([] for i in range(5))\n",
    "    for _, row in df.iterrows():\n",
    "        slug = row['Slug']\n",
    "        ## access the download link\n",
    "        if row['Format'] == 'Imagery':\n",
    "            url = row['ImageServer']\n",
    "        else:\n",
    "            url = row['Download']\n",
    "        try:\n",
    "            ## set timeout to avoid waiting for the server to response forever             \n",
    "            response = requests.get(url, timeout = timeout, proxies = urllib.request.getproxies())\n",
    "            response.raise_for_status()\n",
    "            ## vector data: check if it is a shapefile  \n",
    "            ## only keep the data source url\n",
    "            if 'content-type' in response.headers and response.headers['content-type'] == 'application/json; charset=utf-8':\n",
    "                countnotshp += 1\n",
    "                oklist.append(slug)\n",
    "                print(f'{slug}: Not a shapefile')\n",
    "            ## imagery Data: check if we could access ImageServer\n",
    "            ## only keep the data source url\n",
    "            elif 'Cache-Control' in response.headers and response.headers['Cache-Control'] == 'private':\n",
    "                countprivate += 1\n",
    "                oklist.append(slug)\n",
    "                print(f'{slug}: Could not access ImageServer')  \n",
    "            else:\n",
    "                ## if records with both vaild data source page and download link\n",
    "                ## query the file size and keep both links\n",
    "                if 'content-length' in response.headers:\n",
    "                    filesize = str(round(int(response.headers['content-length']) / 1000000, 4)) + ' MB'\n",
    "                if row['Download'] is not None:\n",
    "                    download = row['Download']\n",
    "                if row['ImageServer'] is not None:\n",
    "                    imageserver = row['ImageServer']\n",
    "                countok += 1\n",
    "                oklist.append(slug)\n",
    "                print(f'{slug}: Success')\n",
    "        ## check HTTP error: 404 (not found) or 500 (server error)       \n",
    "        except requests.exceptions.HTTPError as errh:\n",
    "            ## 404 error: drop this record\n",
    "            if errh.response.status_code == 404:\n",
    "                count404 += 1\n",
    "            ## 500 error: only keeps data source url    \n",
    "            elif errh.response.status_code == 500:\n",
    "                count500 += 1\n",
    "                oklist.append(slug)\n",
    "            ## other HTTP errors: only keeps data source url\n",
    "            else:\n",
    "                countothercode += 1\n",
    "            print (f'{slug}: {errh}')\n",
    "        ## check Connection error: need to be double-checked by increasing the timeout or even manually open it    \n",
    "        except requests.exceptions.ConnectionError as errc:\n",
    "            download = row['Download']\n",
    "            imageserver = row['ImageServer']\n",
    "            countconnection += 1\n",
    "            checkagainlist.append(slug)\n",
    "            print (f'{slug}: {errc}')\n",
    "        ## check Timeout error: need to be double-checked by increasing the timeout or even manually open it \n",
    "        except requests.exceptions.Timeout as errt:\n",
    "            download = row['Download']\n",
    "            imageserver = row['ImageServer']            \n",
    "            counttimeout += 1\n",
    "            checkagainlist.append(slug)\n",
    "            print (f'{slug}: {errt}')\n",
    "        \n",
    "        filesizelist.append(filesize)\n",
    "        downloadlist.append(download)\n",
    "        imageserverlist.append(imageserver)\n",
    "    \n",
    "    df['FileSize'] = filesizelist    \n",
    "    df['Download'] = downloadlist \n",
    "    df['ImageServer'] = imageserverlist \n",
    "    \n",
    "    errordict = {'OK': countok, 'Not a shapefile': countnotshp, 'Timeout Error': counttimeout,\n",
    "                 'Could not access ImageServer': countprivate, '404 Not Found': count404, \n",
    "                 '500 Internal Server Error': count500, 'Other HTTP Errors': countothercode, \n",
    "                 'Connection Errors': countconnection}     \n",
    "    msglist = [f'{k}: {v}, {round(v/totalcount * 100.0, 2)}%' for k, v in errordict.items()]\n",
    "    print('\\n---------- %s seconds ----------' % round((time.time() - start_time), 0), \n",
    "          '\\n\\n---------- Error Summary ----------', \n",
    "          '\\nAll records: %s' % totalcount)\n",
    "    for msg in msglist:\n",
    "        print(msg)\n",
    "    \n",
    "    ## records with runtime error need to be double-checked\n",
    "    return [df[df['Slug'].isin(oklist)], df[df['Slug'].isin(checkagainlist)]]\n",
    "\n",
    "df_total = check_url(df_csv, 3)\n",
    "df_ok = df_total[0].reset_index(drop = True)\n",
    "df_checkagain = df_total[1].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the timeout as 10 seconds\n",
    "## if there still exists any records, manually check the download link to see if it works\n",
    "if len(df_checkagain.index):\n",
    "    df_checkagain = check_url(df_checkagain, 10)\n",
    "    df_checkok = df_checkagain[0].reset_index(drop = True)\n",
    "    df_manualcheck = df_checkagain[1].reset_index(drop = True)\n",
    "    df_manualcheck['Title'] = 'Manually check this link!'\n",
    "    df_csv = pd.concat([df_ok, df_checkok, df_manualcheck]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Split csv file if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if records come from Esri, the spatial coverage is considered as United States\n",
    "df_esri = df_csv[df_csv['Publisher'] == 'Esri'].reset_index(drop=True)\n",
    "df_csv = df_csv[df_csv['Publisher'] != 'Esri'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Splite state from column 'Publisher'\n",
    "The portal code is the main indicator: <br>\n",
    "- 01 - Indiana\n",
    "- 02 - Illinois\n",
    "- 03 - Iowa\n",
    "- 04 - Maryland\n",
    "- 04c-01 - District of Columbia\n",
    "- 04f-01 - Delaware, Philadelphia, Maryland, New Jersey\n",
    "- 05 - Minnesota\n",
    "- 06 - Michigan\n",
    "- 07 - Michigan\n",
    "- 08 - Pennsylvania\n",
    "- 09 - Indiana\n",
    "- 10 - Wisconsin\n",
    "- 11 - Ohio\n",
    "- 12 - Illinois\n",
    "- 13 - Nebraska\n",
    "- 99 - Esri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--------------------- Populating spatial coverage based on bounding box: --------------------\\n\")\n",
    "\n",
    "statedict = {'01': 'Indiana', '02': 'Illinois', '03': 'Iowa', '04': 'Maryland', '04c-01': 'District of Columbia', \n",
    "             '04f-01': '04f-01', '05': 'Minnesota', '06': 'Michigan', '07': 'Michigan', '08': 'Pennsylvania', \n",
    "             '09': 'Indiana', '10': 'Wisconsin', '11': 'Ohio', '12': 'Illinois', '13': 'Nebraska', '99': 'Esri'}\n",
    "\n",
    "df_csv['State'] = [statedict[row['Code']] if row['Code'] in statedict.keys(\n",
    "                    ) else statedict[row['Code'][0:2]] for _, row in df_csv.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Build up GeoJSON dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create bounding box for csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_coordinates(df, identifier):\n",
    "    ## create regular bouding box coordinate pairs and round them to 2 decimal places\n",
    "    ## manually generates the buffering zone\n",
    "    df = pd.concat([df, df['Bounding Box'].str.split(',', expand=True).astype(float).round(2)], axis=1).rename(\n",
    "        columns={0:'minX', 1:'minY', 2:'maxX', 3:'maxY'})\n",
    "    \n",
    "    ## check if there exists wrong coordinates and drop them\n",
    "    coordslist = ['minX', 'minY', 'maxX', 'maxY']\n",
    "    idlist = []\n",
    "    for _, row in df.iterrows():\n",
    "        for coord in coordslist:\n",
    "            ## e.g. [-180.0000,-90.0000,180.0000,90.0000]             \n",
    "            if abs(row[coord]) == 0 or abs(row[coord]) == 180:\n",
    "                idlist.append(row[identifier])\n",
    "        if (row.maxX - row.minX) > 10 or (row.maxY - row.minY) > 10:\n",
    "            idlist.append(row[identifier])\n",
    "    \n",
    "    ## create bounding box\n",
    "    df['Coordinates'] = df.apply(lambda row: box(row.minX, row.minY, row.maxX, row.maxY), axis=1)\n",
    "    df['Roundcoords'] = df.apply(lambda row: ', '.join([str(i) for i in [row.minX, row.minY, row.maxX, row.maxY]]), axis=1)\n",
    "    \n",
    "    ## clean up unnecessary columns\n",
    "    df = df.drop(columns = coordslist).reset_index(drop = True)\n",
    "    \n",
    "    df_clean = df[~df[identifier].isin(idlist)]\n",
    "    ## remove records with wrong coordinates into a new dataframe\n",
    "    df_wrongcoords = df[df[identifier].isin(idlist)].drop(columns = ['State', 'Coordinates'])\n",
    "    \n",
    "    return [df_clean, df_wrongcoords]\n",
    "\n",
    "df_csvlist = format_coordinates(df_csv, 'Slug')\n",
    "df_clean = df_csvlist[0]\n",
    "df_wrongcoords = df_csvlist[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Convert csv and GeoJSON file into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_rawdata = gpd.GeoDataFrame(df_clean, geometry = df_clean['Coordinates'])\n",
    "gdf_rawdata.crs = 'EPSG:4326'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Split dataframe and convert them into dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## e.g.\n",
    "## splitdict = {'Minnesota': {'Roundcoords 1': df_1, 'Roundcoords 2': df_2}, \n",
    "##              'Michigan':  {'Roundcoords 3': df_3, ...}, \n",
    "##               ...}\n",
    "\n",
    "splitdict = {}\n",
    "for state in list(gdf_rawdata['State'].unique()):\n",
    "    gdf_slice = gdf_rawdata[gdf_rawdata['State'] == state]\n",
    "    if state:\n",
    "        coordsdict = {}\n",
    "        for coord in list(gdf_slice['Roundcoords'].unique()):\n",
    "            coordsdict[coord] = gdf_slice[gdf_slice['Roundcoords'] == coord].drop(columns = ['State', 'Roundcoords'])\n",
    "        splitdict[state] = coordsdict\n",
    "    else:\n",
    "        sluglist = gdf_slice['Code'].unique()\n",
    "        print(\"Can't find the bounding box file: \", sluglist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Spatial Join\n",
    "**<a href='https://geopandas.org/reference/geopandas.sjoin.html#geopandas-sjoin'>`geopandas.sjoin`</a>** provides the following the criteria used to match rows:\n",
    "- intersects \n",
    "- within\n",
    "- contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Perform spatial join on each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_placename(df, level):\n",
    "    formatlist = []\n",
    "    for _, row in df.iterrows():\n",
    "        ## e.g. 'Baltimore County, Baltimore City'\n",
    "        ## --> ['Baltimore County&Maryland', 'Baltimore City&Maryland']\n",
    "        if row[level] != 'nan':\n",
    "            placelist = row[level].split(', ')\n",
    "            formatname = ', '.join([(i + '&' + row['State']) for i in placelist])  \n",
    "        ## e.g. 'nan'\n",
    "        ## --> ['nan']\n",
    "        else:\n",
    "            formatname = 'nan'\n",
    "        formatlist.append(formatname)\n",
    "    return formatlist\n",
    "\n",
    "\"\"\" Spatial Join: both city and county bounding box files \"\"\"\n",
    "def city_and_county_sjoin(gdf_rawdata, op, state):\n",
    "    bboxpath = os.path.join('geojsons', state, f'{state}_City_bbox.json')\n",
    "    gdf_basemap = gpd.read_file(bboxpath)\n",
    "    ## spatial join\n",
    "    df_merged = gpd.sjoin(gdf_rawdata, gdf_basemap, op = op, how = 'left')[['City', 'County', 'State']].astype(str)\n",
    "    # merge column 'City', 'County' into one column 'op'\n",
    "    df_merged['City'] = split_placename(df_merged, 'City')\n",
    "    df_merged['County'] = split_placename(df_merged, 'County')\n",
    "    df_merged[op] = df_merged[['City', 'County']].agg(', '.join, axis=1).replace('nan, nan', 'nan')\n",
    "    # convert placename into list\n",
    "    oplist = df_merged[op].astype(str).values.tolist()\n",
    "    return oplist\n",
    "\n",
    "\"\"\" Spatial Join: either city or county bounding box file \"\"\"\n",
    "def city_or_county_sjoin(gdf_rawdata, op, state, level):\n",
    "    bboxpath = os.path.join('geojsons', state, f'{state}_{level}_bbox.json')\n",
    "    gdf_basemap = gpd.read_file(bboxpath)\n",
    "    ## spatial join\n",
    "    df_merged = gpd.sjoin(gdf_rawdata, gdf_basemap, op = op, how = 'left')[[level, 'State']].astype(str)\n",
    "    # merge column level and 'State' into one column 'op'\n",
    "    df_merged[op] = df_merged.apply(lambda row: (row[level] + '&' + row['State']) if str(row[level]) != 'nan' else 'nan', axis = 1)\n",
    "    # convert placename into list\n",
    "    oplist = df_merged[op].astype(str).values.tolist()\n",
    "    return oplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Remove duplicates and 'nan' from place name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(row):\n",
    "    ## e.g. ['nan', 'Minneapolis, Minnesota', 'Hennepin County, Minnesota', 'Hennepin County, Minnesota']\n",
    "    ## remove 'nan' and duplicates from list: ['Minneapolis, Minnesota, 'Hennepin County, Minnesota']\n",
    "    nonan = list(filter(lambda x: x != 'nan', row))\n",
    "    nodups = list(set(', '.join(nonan).split(', ')))\n",
    "    result = [i.replace('&', ', ') for i in nodups]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Fetch the proper join bouding box files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = ['intersects', 'within', 'contains']\n",
    "def spatial_join(gdf_rawdata, state, length):\n",
    "    oplist = []\n",
    "    for op in operations:\n",
    "        bboxpath = os.path.join('geojsons', state, f'{state}_City_bbox.json')\n",
    "        \n",
    "        ## Disteict of Columbia doesn't have county boudning box file\n",
    "        if state == 'District of Columbia':\n",
    "            columbia = city_or_county_sjoin(gdf_rawdata, op, state, 'City')\n",
    "            pnamelist = remove_nan(columbia)\n",
    "        \n",
    "        ## check if there exists bounding box files\n",
    "        elif os.path.isfile(bboxpath):\n",
    "            city_county_state_list = city_and_county_sjoin(gdf_rawdata, op, state)\n",
    "            county_state_list = city_or_county_sjoin(gdf_rawdata, op, state, 'County')\n",
    "            pnamelist = city_county_state_list + county_state_list\n",
    "            pnamelist = remove_nan(pnamelist)\n",
    "       \n",
    "        ## missing bounding box file    \n",
    "        else: \n",
    "            print('Missing city bounding box file: ', state)\n",
    "            continue \n",
    "        \n",
    "        oplist.append(pnamelist)\n",
    "    ## duplicate placename list for all rows with the same bounding box    \n",
    "    allopslist = list(repeat(oplist, length))\n",
    "    ## ultimately it returns a dataframe with placename related to matching operation\n",
    "    ## e.g. dataframe = {'intersects', 'within', 'contains'}\n",
    "    df_match = pd.DataFrame.from_records(allopslist, columns = operations)\n",
    "    return df_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Merge place names generated by three matching operations to raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeddf = []\n",
    "## loop through splitdict based on key 'State'\n",
    "for state, gdfdict in splitdict.items():\n",
    "    ## loop through records based on key 'Bounding Box'\n",
    "    for coord, gdf_split in gdfdict.items():\n",
    "        length = int(len(gdf_split))\n",
    "        df_comparison = spatial_join(gdf_split.iloc[[0]], state, length)\n",
    "        ## append dataframe {'intersects', 'within', 'contains'} to raw data\n",
    "        gdf_sjoin = pd.concat([gdf_split.reset_index(drop=True), df_comparison.reset_index(drop=True)], axis = 1)\n",
    "        mergeddf.append(gdf_sjoin)\n",
    "\n",
    "## merge all dataframes with different bounding boxes into one\n",
    "gdf_merged = reduce(lambda left, right: left.append(right), mergeddf).reset_index(drop = True)\n",
    "\n",
    "## replace [''] with None\n",
    "for op in operations:\n",
    "    gdf_merged[op] = gdf_merged[op].apply(lambda row: None if row == [''] else row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Populate place names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Format spatial coverage based on GBL Metadata Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## e.g. ['Camden County, New Jersey', 'Delaware County, Pennsylvania', 'Philadelphia County, Pennsylvania']\n",
    "def format_placename(colname):\n",
    "    inv_map = {}\n",
    "    plist = []\n",
    "\n",
    "    ## {'Camden County': 'New Jersey', 'Delaware County': 'Pennsylvania', 'Philadelphia County': 'Pennsylvania'}\n",
    "    namedict = dict(item.split(', ') for item in colname)\n",
    "\n",
    "    ## {'New Jersey': ['Camden County'], 'Pennsylvania': ['Delaware County', 'Philadelphia County']}\n",
    "    for k, v in namedict.items():\n",
    "        inv_map[v] = inv_map.get(v, []) + [k] \n",
    "    \n",
    "    ## ['Camden County, New Jersey|New Jersey', 'Delaware County, Pennsylvania|Philadelphia County, Pennsylvania|Pennsylvania']\n",
    "    for k, v in inv_map.items():\n",
    "        pname = [elem + ', ' + k for elem in v]\n",
    "        pname.append(k)\n",
    "        plist.append('|'.join(pname))\n",
    "\n",
    "    ## Camden County, New Jersey|New Jersey|Delaware County, Pennsylvania|Philadelphia County, Pennsylvania|Pennsylvania\n",
    "    return '|'.join(plist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Select spatial coverage based on operaions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_placename(df, identifier):\n",
    "    placenamelist = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['contains'] is None:\n",
    "            if row['intersects'] is None: \n",
    "                placename = ''\n",
    "            elif row['within'] is None:\n",
    "                placename = format_placename(row['intersects']) \n",
    "            else: \n",
    "                placename = format_placename(row['within']) \n",
    "        else:\n",
    "            placename = format_placename(row['contains']) \n",
    "        placenamelist.append(placename)\n",
    "    df['Spatial Coverage'] = placenamelist\n",
    "    return df.drop(columns = ['intersects', 'within', 'contains', 'Coordinates', 'geometry'])\n",
    "\n",
    "df_bbox = populate_placename(gdf_merged, 'Slug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Write to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if there exists data portal from Esri\n",
    "if len(df_esri):\n",
    "    df_esri['Spatial Coverage'] = 'United States'\n",
    "    \n",
    "dflist = [df_esri, df_bbox, df_wrongcoords]\n",
    "df_final = pd.concat(filter(len, dflist), ignore_index=True)\n",
    "\n",
    "df_final.to_csv(newItemscsv, index = False)\n",
    "\n",
    "print(\"\\n--------------------- Congrats! ╰(￣▽￣)╯ --------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
