{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populating Spatial Coverage for DCAT Data Portals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction\n",
    "This Jupyter Notebook is intended to find the spatial coverage based on bounding box for DCAT Data Portals. It is a reverse version of **<a href='https://github.com/BTAA-Geospatial-Data-Project/geonames'>geonames</a>**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparation\n",
    "We will be using **Jupyter Notebook(anaconda 3)** to edit and run the script. Information on Anaconda installation can be found <a href='https://docs.anaconda.com/anaconda/install/'>here</a>. Please note that this script is running on Python 3.\n",
    "\n",
    "Before running the script, you may need to:\n",
    "### 1. Run other two Jupyter Notebooks\n",
    "- If the target state(s) hasn't been converted into city or county bounding box file, you may need to run `city_bbox.ipynb` or `county_bbox.ipynb` or `merge_geojson.ipynb` first.  \n",
    "\n",
    "### 2. Restructure Directories\n",
    "- `dcat_sjoin.ipynb`\n",
    "- `city_bbox.ipynb`\n",
    "- `county_bbox.ipynb`\n",
    "- `merge_geojson.ipynb`\n",
    "- geojson folder\n",
    "    - State1 foloder\n",
    "        - *State1_County_bbox.json*\n",
    "        - *State1_City_bbox.json*\n",
    "    - State2 foloder\n",
    "        - *State2_County_bbox.json*\n",
    "        - *State2_City_bbox.json*\n",
    "    - ...\n",
    "- reports folder\n",
    "    - *allNewItems_ActionDate.csv* formatted in GBL Metadata Template\n",
    "        \n",
    "### 3. Inspect *allNewItems_ActionDate.csv*\n",
    "If records belong to regional data portal, you probably need to create merged county bounding box file first. \n",
    "\n",
    "The final product would be one CSV file named ***allNewItems_ActionDate_test.csv***. \n",
    "\n",
    "> Original created on Feb 4 2021 <br>\n",
    "> @author: Yijing Zhou @YijingZhou33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from shapely.geometry import box\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import time\n",
    "import urllib.request\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActionDate = time.strftime('%Y%m%d')\n",
    "# newItemscsv = os.path.join('reports', f'allNewItems_{ActionDate}.csv')\n",
    "newItemscsv = os.path.join('reports', 'allNewItems_20210202.csv')\n",
    "df_csv = pd.read_csv(newItemscsv, encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Check if download link is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a zip file:  ec8e015cb6d941db94d1c963d1355526_4\n",
      "Not a zip file:  ec8e015cb6d941db94d1c963d1355526_3\n",
      "HTTPSConnectionPool(host='data2018-mcgov-gis.opendata.arcgis.com', port=443): Read timed out. (read timeout=3): 42f9c2daa9824d3dabd6714e493f2b88_0\n",
      "HTTPSConnectionPool(host='data2018-mcgov-gis.opendata.arcgis.com', port=443): Read timed out. (read timeout=3): b47d9884847d4e6a8a06766e636bb9bd_0\n",
      "HTTPSConnectionPool(host='data2018-mcgov-gis.opendata.arcgis.com', port=443): Read timed out. (read timeout=3): 16343b687c624a5aa3b658ea18d4cb26_0\n",
      "HTTPSConnectionPool(host='data.baltimorecity.gov', port=443): Read timed out. (read timeout=3): 8218ff5808d94165baa89afd46587f52_0\n",
      "500 Server Error: Internal Server Error for url: https://opendata.minneapolismn.gov/datasets/d3c37bd37e8f4e7a92b844710045629d_0.zip: d3c37bd37e8f4e7a92b844710045629d_0\n",
      "Not a zip file:  4fb8522d16164670b43a3434b73b9fbf_0\n",
      "Not a zip file:  317bf35dd9b24a49a092c00c83a81fff_0\n",
      "Not a zip file:  c47ec477a9514df4afac431292c10ad0_0\n",
      "Not a zip file:  e9d334031c114e87be9b7edd17c978c3_0\n",
      "Not a zip file:  a9c17397c0b840b5993c5f9fccb6c6a4_33\n",
      "Not a zip file:  771e657ed2b54767b8b025cfe1af5b26_0\n",
      "HTTPSConnectionPool(host='opendata.dc.gov', port=443): Read timed out. (read timeout=3): 43e7da983bd24f8cbaceb6e654f0da3d_42\n",
      "Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?: f24113d342d343fea76fbb771aaedca0\n",
      "Not a zip file:  d980f82dc38149ea933636bf35e99af4_0\n",
      "HTTPSConnectionPool(host='public-iowadot.opendata.arcgis.com', port=443): Read timed out. (read timeout=3): b30fc1e594e84d509f3e14cabb4b2824_4\n",
      "Not a zip file:  b30fc1e594e84d509f3e14cabb4b2824_5\n",
      "Not a zip file:  b30fc1e594e84d509f3e14cabb4b2824_1\n",
      "Not a zip file:  b30fc1e594e84d509f3e14cabb4b2824_6\n",
      "404 Client Error: Not Found for url: https://www.nebraskamap.gov/datasets/e7ee9d80bafa483781db5bcc7cc79da4_0.zip: e7ee9d80bafa483781db5bcc7cc79da4_0\n",
      "404 Client Error: Not Found for url: https://www.nebraskamap.gov/datasets/50b36669579547c89dcb3ed367658c8f_0.zip: 50b36669579547c89dcb3ed367658c8f_0\n"
     ]
    }
   ],
   "source": [
    "def check_download(df):\n",
    "    sluglist = []\n",
    "    for _, row in df.iterrows():\n",
    "        url = row['Download']\n",
    "        slug = row['Slug']\n",
    "        try:\n",
    "            response = requests.get(url, timeout = 3)\n",
    "            response.raise_for_status()\n",
    "            if response.headers['content-type'] == 'application/json; charset=utf-8':\n",
    "                print(f'{slug}: Not a zipfile')\n",
    "            else:\n",
    "                sluglist.append(slug)\n",
    "        except requests.exceptions.HTTPError as errh:\n",
    "            print (f'{slug}: {errh}')\n",
    "        except requests.exceptions.RequestException as err:\n",
    "            print (f'{slug}: {err}')\n",
    "        except requests.exceptions.ConnectionError as errc:\n",
    "            print (f'{slug}: {errc}')\n",
    "        except requests.exceptions.Timeout as errt:\n",
    "            print (f'{slug}: {errt}')\n",
    "    return sluglist  \n",
    "sluglist = check_download(df_csv)\n",
    "df_csv = df_csv[df_csv['Slug'].isin(sluglist)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Split csv file if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4c305304abf7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## If records come from Esri, the spatial coverage is considered as United States.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_esri\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_csv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_csv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Publisher'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Esri'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_csv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_csv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_csv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Publisher'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'Esri'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_csv' is not defined"
     ]
    }
   ],
   "source": [
    "## If records come from Esri, the spatial coverage is considered as United States.\n",
    "df_esri = df_csv[df_csv['Publisher'] == 'Esri'].reset_index(drop=True)\n",
    "df_csv = df_csv[df_csv['Publisher'] != 'Esri'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Classify portals\n",
    "The portal code is the main indicator: <br>\n",
    "- a - state\n",
    "- b - county\n",
    "- c - city\n",
    "- d - university (usually a city)\n",
    "- f - regional\n",
    "- 99 - Esri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portal_level(df):\n",
    "    leveldict = {'a': 'County', 'b': 'County', 'c': 'City', 'd': 'City', 'f': 'Regional'}\n",
    "    levellist = []\n",
    "    statelist = []\n",
    "    for _, row in df.iterrows():\n",
    "        ## if it is a state('a') or county('b') data portal, \n",
    "        ## use county-level bounding box files  \n",
    "        if 'a' in row['Code']:\n",
    "            level = leveldict['a']\n",
    "            state = row['Publisher'].split(' ')[-1] \n",
    "        elif 'b' in row['Code']:\n",
    "            level = leveldict['b']\n",
    "            state = row['Publisher'].split(', ')[-1] \n",
    "            \n",
    "        ## if it is a city('c') or university('d') data portal, \n",
    "        ## use both county-level and county-level bounding box files    \n",
    "        elif 'c' in row['Code']:\n",
    "            level = leveldict['c']\n",
    "            state = row['Publisher'].split(', ')[-1]    \n",
    "        elif 'd' in row['Code']:\n",
    "            level = leveldict['d']\n",
    "            state = row['Publisher'].split(' ')[-1]\n",
    "            \n",
    "        ## if it is a regional('f') data portal, \n",
    "        ## use (merged) county-level bounding box files    \n",
    "        elif 'f' in row['Code']:\n",
    "            ## Regional portal: SEMCOG, Southeast Michigan Council of Governments             \n",
    "            if row['Code'] == '06f-01':\n",
    "                level = leveldict['a']\n",
    "                state = 'Michigan'\n",
    "            ## Regional portal: Delaware Valley Regional Planning Commission\n",
    "            ## The bouding box includes counties from Delawasre, Maryland, New Jersey and Pennsylvania\n",
    "            elif row['Code'] == '04f-01': \n",
    "                level = leveldict['a']\n",
    "                state = 'Delaware'\n",
    "        levellist.append(level)\n",
    "        statelist.append(state)\n",
    "    \n",
    "    df['Level'] = levellist\n",
    "    df['State'] = statelist\n",
    "    return df\n",
    "df_csv = portal_level(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Build up GeoJSON dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create bounding box for csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_coordinates(df, identifier):\n",
    "    ## create regular bouding box coordinate pairs and round them to 2 decimal places\n",
    "    df = pd.concat([df, df['Bounding Box'].str.split(',', expand=True).astype(float).round(2)], axis=1).rename(\n",
    "        columns={0:'minX', 1:'minY', 2:'maxX', 3:'maxY'})\n",
    "    \n",
    "    ## check if there exists wrong coordinates\n",
    "    for _, row in df.iterrows():\n",
    "        if (row.maxX - row.minX) > 10 or (row.maxY - row.minY) > 10:\n",
    "            print(f'Wrong Coordinates --> {identifier}: ', row[identifier])\n",
    "    \n",
    "    ## create bouding box\n",
    "    df['Coordinates'] = df.apply(lambda row: box(row.minX, row.minY, row.maxX, row.maxY), axis=1)\n",
    "    \n",
    "    ## clean up unnecessary columns\n",
    "    return df.drop(columns =['minX', 'minY', 'maxX', 'maxY'])\n",
    "\n",
    "df_clean = format_coordinates(df_csv, 'Slug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Convert csv and GeoJSON file into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_rawdata = gpd.GeoDataFrame(df_clean, geometry = df_clean['Coordinates'])\n",
    "gdf_rawdata.crs = 'EPSG:4326'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Split dataframe and convert them into dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## e.g.\n",
    "## splitdict = {'Minnesota': {'County': df_1, 'City': df_2}, \n",
    "##              'Michigan':  {'County': df_3}, \n",
    "##               ...}\n",
    "\n",
    "splitdict = {}\n",
    "for state in list(gdf_rawdata['State'].unique()):\n",
    "    gdf_slice = gdf_rawdata[gdf_rawdata['State'] == state]\n",
    "    if state:\n",
    "        leveldict = {}\n",
    "        for level in list(gdf_slice['Level'].unique()):\n",
    "            leveldict[level] = gdf_slice[gdf_slice['Level'] == level].drop(columns = 'State')\n",
    "        splitdict[state] = leveldict\n",
    "    else:\n",
    "        df_nobbox = gdf_slice.drop(columns =['Coordinates', 'geometry', 'State'])\n",
    "        sluglist = df_nobbox['Code'].unique()\n",
    "        print(\"Can't find the bounding box file: \", sluglist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Spatial Join\n",
    "**<a href='https://geopandas.org/reference/geopandas.sjoin.html#geopandas-sjoin'>`geopandas.sjoin`</a>** provides the following the criteria used to match rows:\n",
    "- intersects \n",
    "- within\n",
    "- contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Perform spatial Join on each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin(gdf_rawdata, op, state, identifier, level):\n",
    "    bboxpath = os.path.join('geojson', state, f'{state}_{level}_bbox.json')\n",
    "    gdf_basemap = gpd.read_file(bboxpath)\n",
    "    ## spatial join\n",
    "    df_merged = gpd.sjoin(gdf_rawdata, gdf_basemap, op = op, how = 'left')[[identifier, level, 'State']].astype(str)\n",
    "    # merge column level and 'State' into one column 'Placename'\n",
    "    df_merged['Pname'] = df_merged[[level, 'State']].agg(', '.join, axis=1).replace('nan, nan', 'nan')\n",
    "    # group records by identifier\n",
    "    df_group = df_merged.drop(columns = ['State']).reset_index(drop = True).groupby(identifier\n",
    "            )['Pname'].apply(list).reset_index(name = op)\n",
    "    return df_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Format place names from city-level data portals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_city_placename(row, state):\n",
    "    ## replace ['nan, nan'] with ['nan']\n",
    "    if len(row) == 2 and row[0] == 'nan':\n",
    "        result = ['nan']\n",
    "    else:\n",
    "        ## e.g. ['nan', 'Minneapolis, Minnesota, Hennepin County, Minnesota']\n",
    "        ## remove 'nan' from list: ['Minneapolis, Minnesota, Hennepin County, Minnesota']\n",
    "        nonan = filter(lambda x: x != 'nan', row)\n",
    "        ## ['Minneapolis, ', 'Hennepin County, Minnesota']\n",
    "        namelist = ', '.join(nonan).split(state + ', ')\n",
    "        ## ['Minneapolis, Minnesota', 'Hennepin County, Minnesota']\n",
    "        result = list(set([i + state for i in namelist[:-1]] + [namelist[-1]]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Fetch the proper join bouding box file fro different data portals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_join(gdf_rawdata, state, identifier, level):\n",
    "    dflist = []\n",
    "    operations = ['intersects', 'within', 'contains']\n",
    "    for op in operations:\n",
    "        bboxpath = os.path.join('geojson', state, f'{state}_{level}_bbox.json')\n",
    "        \n",
    "        ## city-level records need to perform spatial join twice (city & county)\n",
    "        ## spatial coverage might contain city name\n",
    "        ## e.g. ['Ann Arbor, Michigan', 'Washtenaw County, Michigan']\n",
    "        if level == 'City':\n",
    "            ## Disteict of Columbia doesn't have county boudning box file\n",
    "            if state == 'District of Columbia':\n",
    "                df_group = sjoin(gdf_rawdata, op, state, identifier, level)\n",
    "            ## check if there exists both city and county bouding box file\n",
    "            elif os.path.isfile(bboxpath):\n",
    "                df_city = sjoin(gdf_rawdata, op, state, identifier, 'City')\n",
    "                df_county = sjoin(gdf_rawdata, op, state, identifier, 'County')\n",
    "                df_merged = df_city.append(df_county, ignore_index = True)\n",
    "                df_group = df_merged.groupby(identifier).agg(lambda row: [', '.join(x) for x in row]).reset_index()\n",
    "                df_group[op] = df_group[op].apply(lambda row: format_city_placename(row, state))   \n",
    "            ## missing city file: Iowa & Nebraska    \n",
    "            else: \n",
    "                df_group = sjoin(gdf_rawdata, op, state, identifier, 'County')\n",
    "                \n",
    "        ## county-level records need to perform spatial join once (county)        \n",
    "        elif level == 'County':\n",
    "            df_group = sjoin(gdf_rawdata, op, state, identifier, level)\n",
    "        \n",
    "        ## replace ['nan'] with None\n",
    "        df_group[op] = df_group[op].apply(lambda row: None if row[0] == 'nan' else row)\n",
    "        dflist.append(df_group)\n",
    "\n",
    "    ## merge dataframes created by different match options\n",
    "    df_sjoin = reduce(lambda left,right: pd.merge(left, right, on = identifier, how = 'outer'), dflist)\n",
    "    \n",
    "    ## ultimately it returns a dataframe with identifier and placename related to matching operation\n",
    "    ## e.g. dataframe = {'identifier', 'level', intersects'}\n",
    "    return gdf_rawdata.merge(df_sjoin, on = identifier).drop(columns =['Coordinates', 'geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Merge place names generated by three matching operations to raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeddf = []\n",
    "## loop through splitdict based on key 'State'\n",
    "for state, gdfdict in splitdict.items():\n",
    "    ## loop through records based on key 'Level'\n",
    "    for level, gdf_split in gdfdict.items():\n",
    "        df_comparison = spatial_join(gdf_split, state, 'Slug', level)\n",
    "        ## e.g. mergeddf = {'identifier', 'intersects', 'within', 'contains'}\n",
    "        mergeddf.append(df_comparison)\n",
    "    \n",
    "## merge placename columns ['intersects', 'within', 'contains'] to raw data\n",
    "gdf_merged = reduce(lambda left, right: left.append(right), mergeddf).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Populate place names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Format spatial coverage based on GBL Metadata Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## e.g. ['Camden County, New Jersey', 'Delaware County, Pennsylvania', 'Philadelphia County, Pennsylvania']\n",
    "def format_placename(colname):\n",
    "    inv_map = {}\n",
    "    plist = []\n",
    "    \n",
    "    ## {'Camden County': 'New Jersey', 'Delaware County': 'Pennsylvania', 'Philadelphia County': 'Pennsylvania'}\n",
    "    namedict = dict(item.split(', ') for item in colname)\n",
    "\n",
    "    ## {'New Jersey': ['Camden County'], 'Pennsylvania': ['Delaware County', 'Philadelphia County']}\n",
    "    for k, v in namedict.items():\n",
    "        inv_map[v] = inv_map.get(v, []) + [k] \n",
    "    \n",
    "    ## ['Camden County, New Jersey|New Jersey', 'Delaware County, Pennsylvania|Philadelphia County, Pennsylvania|Pennsylvania']\n",
    "    for k, v in inv_map.items():\n",
    "        pname = [elem + ', ' + k for elem in v]\n",
    "        pname.append(k)\n",
    "        plist.append('|'.join(pname))\n",
    "\n",
    "    ## Camden County, New Jersey|New Jersey|Delaware County, Pennsylvania|Philadelphia County, Pennsylvania|Pennsylvania\n",
    "    return '|'.join(plist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### step 14: Manual items to change \n",
    "Usually if one records intersects too many places, the script will treat the spatial coverage as the whole state. <br>\n",
    "But you can customize it here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Twin Cities Metropolitan Area, Minnesota \n",
    "twin_cities = ['Anoka County, Minnesota', 'Carver County, Minnesota', 'Chisago County, Minnesota', \n",
    "               'Dakota County, Minnesota', 'Hennepin County, Minnesota', 'Ramsey County, Minnesota', \n",
    "               'Scott County, Minnesota', 'Washington County, Minnesota']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 15: Populate spatial coverage for state, county and regional data portals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def county_level_formatting(row):\n",
    "    if row['within'] is None:\n",
    "        ## no within feature && <= 5 contain features --> contains features\n",
    "        if row['contains'] is not None and len(row['contains']) < 6:\n",
    "            placename = format_placename(row['contains'])\n",
    "        ## no intersect, within (and contains) feature --> wrong coordinates\n",
    "        elif row['intersects'] is None:\n",
    "            placename = ''\n",
    "        ## no within feature && > 5 contain features --> state\n",
    "        else:\n",
    "            statedict = dict(item.split(', ') for item in row['intersects']) \n",
    "            placename = ('|').join(set([v for k, v in statedict.items()]))\n",
    "    else:\n",
    "        ## otherwise, within features\n",
    "        placename = format_placename(row['within'])  \n",
    "    return placename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 16: Populate spatial coverage for city and university data portals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_level_formatting(row):\n",
    "    if row['within'] is None:\n",
    "        ## no within feature && <= 5 contain features --> contains features\n",
    "        if row['contains'] is not None and len(row['contains']) < 6:\n",
    "            placename = format_placename(row['contains'])   \n",
    "        ## no intersect, within (and contains) feature --> wrong coordinates    \n",
    "        elif row['intersects'] is None:\n",
    "            placename = '' \n",
    "        else:\n",
    "            ## Twin Cities Metropolitan area            \n",
    "            if row['Code'] == '05c-01':\n",
    "                placename = format_placename(twin_cities)\n",
    "            ## no within feature && <= 5 intersect features --> intersect features\n",
    "            elif row['intersects'] is not None and len(row['intersects']) < 6:\n",
    "                placename = format_placename(row['intersects'])  \n",
    "            ## no within feature && > 5 intersect features --> state\n",
    "            else:\n",
    "                statedict = dict(item.split(', ') for item in row['intersects']) \n",
    "                placename = ('|').join(set([v for k, v in statedict.items()]))\n",
    "    else:   \n",
    "        ## within features && <= 4 contains features --> within + contains features\n",
    "        if row['contains'] is not None and len(row['contains']) < 5:\n",
    "            placename = format_placename(row['contains'] + row['within'])\n",
    "        ## within features && <= 5 intersects features --> intersects features    \n",
    "        elif row['intersects'] is not None and len(row['intersects']) < 6:\n",
    "            placename = format_placename(row['intersects'])\n",
    "        else:\n",
    "            placename = format_placename(row['within']) \n",
    "            \n",
    "    return placename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 17: Merge data portals of different levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_placename(df, identifier):\n",
    "    placenamelist = []\n",
    "    for _, row in df.iterrows():\n",
    "        print('identifier --> ', row[identifier])\n",
    "        if row['Level'] == 'County':\n",
    "            placename = county_level_formatting(row)\n",
    "        elif row['Level'] == 'City':\n",
    "            placename = city_level_formatting(row)\n",
    "        placenamelist.append(placename)\n",
    "    \n",
    "    df['Spatial Coverage'] = placenamelist\n",
    "    return df.drop(columns = ['Level', 'intersects', 'within', 'contains'])\n",
    "df_bbox = populate_placename(gdf_merged, 'Slug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Write to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if there exists data portal from Esri\n",
    "if len(df_esri):\n",
    "    df_esri['Spatial Coverage'] = 'United States'\n",
    "    df_final = df_bbox.append(df_esri, ignore_index = True)\n",
    "else:\n",
    "    df_final = df_bbox\n",
    "\n",
    "df_final.to_csv(newItemscsv, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
