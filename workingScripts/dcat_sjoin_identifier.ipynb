{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populating Spatial Coverage for DCAT Data Portals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction\n",
    "This Jupyter Notebook is intended to find the spatial coverage based on bounding box for DCAT Data Portals. It is a reverse version of **<a href='https://github.com/BTAA-Geospatial-Data-Project/geonames'>geonames</a>**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparation\n",
    "We will be using **Jupyter Notebook(anaconda 3)** to edit and run the script. Information on Anaconda installation can be found <a href='https://docs.anaconda.com/anaconda/install/'>here</a>. Please note that this script is running on Python 3.\n",
    "\n",
    "Before running the script, you may need to:\n",
    "### 1. Run other Jupyter Notebooks\n",
    "- If the target state(s) hasn't been converted into city or county bounding box file, you may need to \n",
    "    1. download county and city boundary file (GeoJSON or Shapefile) online\n",
    "    2. run `city_boundary.ipynb` or `county_boundary.ipynb` to create boundary GeoJSON files \n",
    "        - if there exists regional data portals, you may need to run `merge_geojson.ipynb` to merge them together\n",
    "    3. run `city_bbox.ipynb` or `county_bbox.ipynb` to create bounding box GeoJSON files\n",
    "\n",
    "### 2. Restructure Directories\n",
    "- `dcat_sjoin.ipynb`\n",
    "- `city_boundary.ipynb`\n",
    "- `county_boundary.ipynb`\n",
    "- `city_bbox.ipynb`\n",
    "- `county_bbox.ipynb`\n",
    "- `merge_geojson.ipynb`\n",
    "- geojson folder\n",
    "    - State1 foloder\n",
    "        - *State1_County_boundaries.json*\n",
    "        - *State1_City_boundaries.json*\n",
    "        - *State1_County_bbox.json*\n",
    "        - *State1_City_bbox.json*\n",
    "    - Code foloder (Multiple states)\n",
    "        - *Code_County_boundaries.json*\n",
    "        - *Code_City_boundaries.json*\n",
    "        - *Code_County_bbox.json*\n",
    "        - *Code_City_bbox.json*\n",
    "    - ...\n",
    "- reports folder\n",
    "    - *allNewItems_ActionDate.csv* formatted in GBL Metadata Template\n",
    "        \n",
    "### 3. Inspect *allNewItems_ActionDate.csv*\n",
    "The final product would be one CSV file named ***allNewItems_ActionDate_test.csv***. \n",
    "\n",
    "> Original created on Feb 4 2021 <br>\n",
    "> @author: Yijing Zhou @YijingZhou33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from shapely.geometry import box\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "import time\n",
    "import urllib.request\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActionDate = time.strftime('%Y%m%d')\n",
    "newItemscsv = os.path.join('reports', f'allNewItems_{ActionDate}.csv')\n",
    "df_csv = pd.read_csv(newItemscsv, encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Check if download link is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_download(df):\n",
    "    sluglist = []\n",
    "    for _, row in df.iterrows():\n",
    "        url = row['Download']\n",
    "        slug = row['Slug']\n",
    "        try:\n",
    "            response = requests.get(url, timeout = 3)\n",
    "            response.raise_for_status()\n",
    "            ## check if it is a zipfile             \n",
    "            if response.headers['content-type'] == 'application/json; charset=utf-8':\n",
    "                print(f'{slug}: Not a zipfile')\n",
    "            else:\n",
    "                print(f'{slug}: Success')\n",
    "                sluglist.append(slug)\n",
    "        ## check HTTPError: 404(not found) or 500 (server error)       \n",
    "        except requests.exceptions.HTTPError as errh:\n",
    "            print (f'{slug}: {errh}')\n",
    "        except requests.exceptions.RequestException as err:\n",
    "            print (f'{slug}: {err}')\n",
    "        except requests.exceptions.ConnectionError as errc:\n",
    "            print (f'{slug}: {errc}')\n",
    "        ## check Timeout: it will retry connecting 3 times before throwing the error  \n",
    "        except requests.exceptions.Timeout as errt:\n",
    "            attempts = 3\n",
    "            while attempts:\n",
    "                try:\n",
    "                    response = requests.get(url, timeout = 3)\n",
    "                    break\n",
    "                except TimeoutError:\n",
    "                    attempts -= 1\n",
    "            print (f'{slug}: {errt}')\n",
    "    return sluglist  \n",
    "sluglist = check_download(df_csv)\n",
    "\n",
    "## only includes records with valid download link\n",
    "df_csv = df_csv[df_csv['Slug'].isin(sluglist)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Split csv file if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if records come from Esri, the spatial coverage is considered as United States\n",
    "df_esri = df_csv[df_csv['Publisher'] == 'Esri'].reset_index(drop=True)\n",
    "df_csv = df_csv[df_csv['Publisher'] != 'Esri'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Splite state from column 'Publisher'\n",
    "The portal code is the main indicator: <br>\n",
    "- a - state\n",
    "- b - county\n",
    "- c - city\n",
    "- d - university (usually a city)\n",
    "- f - regional\n",
    "- 99 - Esri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_state(df):\n",
    "    statelist = []\n",
    "    for _, row in df.iterrows():\n",
    "        if 'a' in row['Code']:\n",
    "            # e.g. State of Minnesota\n",
    "            state = row['Publisher'].split(' ')[-1] \n",
    "        elif 'b' in row['Code']:\n",
    "            # e.g. Wilkin County, Minnesota\n",
    "            state = row['Publisher'].split(', ')[-1]             \n",
    "        elif 'c' in row['Code']:\n",
    "            # e.g. City of Baltimore, Maryland\n",
    "            state = row['Publisher'].split(', ')[-1]    \n",
    "        elif 'd' in row['Code']:\n",
    "            # e.g. University of Michigan\n",
    "            state = row['Publisher'].split(' ')[-1]           \n",
    "        elif 'f' in row['Code']:\n",
    "            ## Regional portal: SEMCOG, Southeast Michigan Council of Governments             \n",
    "            if row['Code'] == '06f-01':\n",
    "                state = 'Michigan'\n",
    "            ## Regional portal: Delaware Valley Regional Planning Commission\n",
    "            ## The bouding box includes counties from Delawasre, Maryland, New Jersey and Pennsylvania\n",
    "            elif row['Code'] == '04f-01': \n",
    "                state = '04f-01'\n",
    "        statelist.append(state)\n",
    "    \n",
    "    df['State'] = statelist\n",
    "    return df\n",
    "df_csv = split_state(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Build up GeoJSON dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create bounding box for csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_coordinates(df, identifier):\n",
    "    ## create regular bouding box coordinate pairs and round them to 2 decimal places\n",
    "    ## manually generates the buffering zone\n",
    "    df = pd.concat([df, df['Bounding Box'].str.split(',', expand=True).astype(float).round(2)], axis=1).rename(\n",
    "        columns={0:'minX', 1:'minY', 2:'maxX', 3:'maxY'})\n",
    "    \n",
    "    ## check if there exists wrong coordinates and drop them\n",
    "    coordslist = ['minX', 'minY', 'maxX', 'maxY']\n",
    "    idlist = []\n",
    "    for _, row in df.iterrows():\n",
    "        for coord in coordslist:\n",
    "            if abs(row[coord]) == 0 or abs(row[coord]) == 180:\n",
    "                idlist.append(row[identifier])\n",
    "        if (row.maxX - row.minX) > 10 or (row.maxY - row.minY) > 10:\n",
    "            idlist.append(row[identifier])\n",
    "    \n",
    "    ## create bouding box\n",
    "    df['Coordinates'] = df.apply(lambda row: box(row.minX, row.minY, row.maxX, row.maxY), axis=1)\n",
    "    \n",
    "    ## clean up unnecessary columns\n",
    "    df = df.drop(columns =['minX', 'minY', 'maxX', 'maxY']).reset_index(drop = True)\n",
    "    \n",
    "    df_clean = df[~df[identifier].isin(idlist)]\n",
    "    df_wrongcoords = df[df[identifier].isin(idlist)].drop(columns = ['State', 'Coordinates'])\n",
    "    \n",
    "    return [df_clean, df_wrongcoords]\n",
    "\n",
    "df_csvlist = format_coordinates(df_csv, 'Slug')\n",
    "df_clean = df_csvlist[0]\n",
    "df_wrongcoords = df_csvlist[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Convert csv and GeoJSON file into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_rawdata = gpd.GeoDataFrame(df_clean, geometry = df_clean['Coordinates'])\n",
    "gdf_rawdata.crs = 'EPSG:4326'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Split dataframe and convert them into dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## e.g.\n",
    "## splitdict = {'Minnesota': {'Bounding Box 1': df_1, 'Bounding Box 2': df_2}, \n",
    "##              'Michigan':  {'Bounding Box 3': df_3, ...}, \n",
    "##               ...}\n",
    "\n",
    "splitdict = {}\n",
    "for state in list(gdf_rawdata['State'].unique()):\n",
    "    gdf_slice = gdf_rawdata[gdf_rawdata['State'] == state]\n",
    "    if state:\n",
    "        bboxdict = {}\n",
    "        for bbox in list(gdf_slice['Bounding Box'].unique()):\n",
    "            bboxdict[bbox] = gdf_slice[gdf_slice['Bounding Box'] == bbox].drop(columns = 'State')\n",
    "        splitdict[state] = bboxdict\n",
    "    else:\n",
    "        df_nobbox = gdf_slice.drop(columns =['Coordinates', 'geometry', 'State'])\n",
    "        sluglist = df_nobbox['Code'].unique()\n",
    "        print(\"Can't find the bounding box file: \", sluglist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Spatial Join\n",
    "**<a href='https://geopandas.org/reference/geopandas.sjoin.html#geopandas-sjoin'>`geopandas.sjoin`</a>** provides the following the criteria used to match rows:\n",
    "- intersects \n",
    "- within\n",
    "- contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Perform spatial Join on each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_placename(df, level):\n",
    "    formatlist = []\n",
    "    for _, row in df.iterrows():\n",
    "        ## e.g. 'Baltimore County, Baltimore City'\n",
    "        ## --> ['Baltimore County&Maryland', 'Baltimore City&Maryland']\n",
    "        if row[level] != 'nan':\n",
    "            placelist = row[level].split(', ')\n",
    "            formatname = ', '.join([(i + '&' + row['State']) for i in placelist])  \n",
    "        ## e.g. 'nan'\n",
    "        ## --> ['nan']\n",
    "        else:\n",
    "            formatname = 'nan'\n",
    "        formatlist.append(formatname)\n",
    "    return formatlist\n",
    "\n",
    "def city_and_county_sjoin(gdf_rawdata, op, state, identifier):\n",
    "    bboxpath = os.path.join('geojson', state, f'{state}_City_bbox.json')\n",
    "    gdf_basemap = gpd.read_file(bboxpath)\n",
    "    ## spatial join\n",
    "    df_merged = gpd.sjoin(gdf_rawdata, gdf_basemap, op = op, how = 'left')[[identifier, 'City', 'County', 'State']].astype(str)\n",
    "    # merge column 'City', 'County' into one column 'Pname'\n",
    "    df_merged['City'] = split_placename(df_merged, 'City')\n",
    "    df_merged['County'] = split_placename(df_merged, 'County')\n",
    "    df_merged['Pname'] = df_merged[['City', 'County']].agg(', '.join, axis=1).replace('nan, nan', 'nan')\n",
    "    # group records by identifier\n",
    "    df_group = df_merged.drop(columns = ['State']).reset_index(drop = True).groupby(identifier\n",
    "            )['Pname'].apply(list).reset_index(name = op)\n",
    "    return df_group\n",
    "\n",
    "def city_or_county_sjoin(gdf_rawdata, op, state, identifier, level):\n",
    "    bboxpath = os.path.join('geojson', state, f'{state}_{level}_bbox.json')\n",
    "    gdf_basemap = gpd.read_file(bboxpath)\n",
    "    ## spatial join\n",
    "    df_merged = gpd.sjoin(gdf_rawdata, gdf_basemap, op = op, how = 'left')[[identifier, level, 'State']].astype(str)\n",
    "    # merge column level and 'State' into one column 'Placename'\n",
    "    df_merged['Pname'] = df_merged.apply(lambda row: (row[level] + '&' + row['State']) if str(row[level]) != 'nan' else 'nan', axis = 1)\n",
    "    # group records by identifier\n",
    "    df_group = df_merged.drop(columns = ['State']).reset_index(drop = True).groupby(identifier\n",
    "            )['Pname'].apply(list).reset_index(name = op)\n",
    "    return df_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Remove duplicates and 'nan' from place name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(row):\n",
    "    ## e.g. ['nan', 'Minneapolis, Minnesota', 'Hennepin County, Minnesota', 'Hennepin County, Minnesota']\n",
    "    ## remove 'nan' and duplicates from list: ['Minneapolis, Minnesota, 'Hennepin County, Minnesota']\n",
    "    nonan = list(filter(lambda x: x != 'nan', row))\n",
    "    nodups = list(set(', '.join(nonan).split(', ')))\n",
    "    result = [i.replace('&', ', ') for i in nodups]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Fetch the proper join bouding box files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_join(gdf_rawdata, state, identifier):\n",
    "    dflist = []\n",
    "    operations = ['intersects', 'within', 'contains']\n",
    "    for op in operations:\n",
    "        bboxpath = os.path.join('geojson', state, f'{state}_City_bbox.json')\n",
    "        \n",
    "        ## Disteict of Columbia doesn't have county boudning box file\n",
    "        if state == 'District of Columbia':\n",
    "            df_group = city_or_county_sjoin(gdf_rawdata, op, state, identifier, 'City')\n",
    "            df_group[op] = df_group[op].apply(lambda row: remove_nan(row)) \n",
    "        \n",
    "        ## check if there exists bounding box files\n",
    "        elif os.path.isfile(bboxpath):\n",
    "            df_city = city_and_county_sjoin(gdf_rawdata, op, state, identifier)\n",
    "            df_county = city_or_county_sjoin(gdf_rawdata, op, state, identifier, 'County')\n",
    "            df_merged = df_city.append(df_county, ignore_index = True)\n",
    "            df_group = df_merged.groupby(identifier).agg(lambda row: [', '.join(x) for x in row]).reset_index()\n",
    "            df_group[op] = df_group[op].apply(lambda row: remove_nan(row))   \n",
    "       \n",
    "        ## missing bounding box file    \n",
    "        else: \n",
    "            print('Missing city bounding box file: ', state)\n",
    "            continue \n",
    "                     \n",
    "        ## replace [''] with None\n",
    "        df_group[op] = df_group[op].apply(lambda row: None if row == [''] else row)\n",
    "        dflist.append(df_group)\n",
    "\n",
    "    ## merge dataframes created by different match options\n",
    "    df_sjoin = reduce(lambda left,right: pd.merge(left, right, on = identifier, how = 'outer'), dflist)\n",
    "    \n",
    "    ## ultimately it returns a dataframe with identifier and placename related to matching operation\n",
    "    ## e.g. dataframe = {'identifier', 'level', intersects'}\n",
    "    gdf_final =  gdf_rawdata.merge(df_sjoin, on = identifier).drop(columns =['Coordinates', 'geometry'])\n",
    "    return gdf_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Merge place names generated by three matching operations to raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeddf = []\n",
    "## loop through splitdict based on key 'State'\n",
    "for state, gdfdict in splitdict.items():\n",
    "    ## loop through records based on key 'Bounding Box'\n",
    "    for bbox, gdf_split in gdfdict.items():\n",
    "        df_comparison = spatial_join(gdf_split, state, 'Slug')\n",
    "        ## e.g. mergeddf = {'identifier', 'intersects', 'within', 'contains'}\n",
    "        mergeddf.append(df_comparison)\n",
    "    \n",
    "## merge placename columns ['intersects', 'within', 'contains'] to raw data\n",
    "gdf_merged = reduce(lambda left, right: left.append(right), mergeddf).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Populate place names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Format spatial coverage based on GBL Metadata Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## e.g. ['Camden County, New Jersey', 'Delaware County, Pennsylvania', 'Philadelphia County, Pennsylvania']\n",
    "def format_placename(colname):\n",
    "    inv_map = {}\n",
    "    plist = []\n",
    "\n",
    "    ## {'Camden County': 'New Jersey', 'Delaware County': 'Pennsylvania', 'Philadelphia County': 'Pennsylvania'}\n",
    "    namedict = dict(item.split(', ') for item in colname)\n",
    "\n",
    "    ## {'New Jersey': ['Camden County'], 'Pennsylvania': ['Delaware County', 'Philadelphia County']}\n",
    "    for k, v in namedict.items():\n",
    "        inv_map[v] = inv_map.get(v, []) + [k] \n",
    "    \n",
    "    ## ['Camden County, New Jersey|New Jersey', 'Delaware County, Pennsylvania|Philadelphia County, Pennsylvania|Pennsylvania']\n",
    "    for k, v in inv_map.items():\n",
    "        pname = [elem + ', ' + k for elem in v]\n",
    "        pname.append(k)\n",
    "        plist.append('|'.join(pname))\n",
    "\n",
    "    ## Camden County, New Jersey|New Jersey|Delaware County, Pennsylvania|Philadelphia County, Pennsylvania|Pennsylvania\n",
    "    return '|'.join(plist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Select spatial coverage based on operaions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_placename(df, identifier):\n",
    "    placenamelist = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['contains'] is None:\n",
    "            if row['intersects'] is None: \n",
    "                placename = ''\n",
    "            elif row['within'] is None:\n",
    "                placename = format_placename(row['intersects']) \n",
    "            else: \n",
    "                placename = format_placename(row['within']) \n",
    "        else:\n",
    "            placename = format_placename(row['contains']) \n",
    "        placenamelist.append(placename)\n",
    "    df['Spatial Coverage'] = placenamelist\n",
    "    return df.drop(columns = ['intersects', 'within', 'contains'])\n",
    "\n",
    "df_bbox = populate_placename(gdf_merged, 'Slug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Write to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if there exists data portal from Esri\n",
    "if len(df_esri):\n",
    "    df_esri['Spatial Coverage'] = 'United States'\n",
    "    \n",
    "dflist = [df_esri, df_bbox, df_wrongcoords]\n",
    "df_final = pd.concat(filter(len, dflist), ignore_index=True)\n",
    "\n",
    "df_final.to_csv(newItemscsv, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
